{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def loss(w, X, y, lam):\n",
    "    z = sigmoid(np.dot(X, w))\n",
    "    return -np.mean(y * np.log(z) + (1-y) * np.log(1-z)) + 0.5*lam/X.shape[0]*np.sum(w*w)\n",
    "def predict(w, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    predict output of each row of X\n",
    "    X: a numpy array of shape\n",
    "    threshold: a threshold between 0 and 1 \n",
    "    \"\"\"\n",
    "    res = np.zeros_like(X.shape[0])\n",
    "    res[np.where(sigmoid(np.dot(X, w)) > threshold)[0]]=1\n",
    "    return res\n",
    "def logistics(w_init, X, y, lam=0.001, lr=0.1, nepoches=2000):\n",
    "    # lam - regularization paramether, lr - learning rate, nepoches - number of epoches\n",
    "    w = w_old = w_init\n",
    "    # store history of loss in loss_hist\n",
    "    loss_hist = [loss(w_init, X, y, lam)]\n",
    "    ep = 0\n",
    "    while ep < nepoches:\n",
    "        mix_idx = np.random.permutation(X.shape[0])\n",
    "        for i in mix_idx:\n",
    "            z = sigmoid(np.dot(X[i], w))\n",
    "            w = w - lr*((z - y[i]) * X[i] + lam*w)\n",
    "        loss_hist.append(loss(w, X, y, lam))\n",
    "        if np.linalg.norm(w - w_old)/X.shape[1] < 1e-6:\n",
    "            break\n",
    "        ep += 1\n",
    "        w_old = w\n",
    "    return w, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, \n",
    "              2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50]]).T\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.54878032 -4.07651591]\n"
     ]
    }
   ],
   "source": [
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "w_init = np.random.randn(Xbar.shape[1])\n",
    "lam=0.0001\n",
    "w, loss_hist = logistics(w_init, Xbar, y, lam, lr=0.05, nepoches=500)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5044048218797967, -4.077003497988468]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=2/lam)\n",
    "model.fit(X, y)\n",
    "print([model.coef_[0, 0], model.intercept_[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5044048218797967"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
