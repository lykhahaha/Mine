{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import *\n",
    "from activation_np import *\n",
    "from gradient_check import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bat data...\n",
      "EOF Reached\n",
      "Done reading\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = get_bat_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProximalAdagradOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_accumulator_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_regularization_strength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_regularization_strength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ProximalAdagrad'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Optimizer that implements the Proximal Adagrad algorithm.\n",
       "\n",
       "See this [paper](http://papers.nips.cc/paper/3793-efficient-learning-using-forward-backward-splitting.pdf).\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Construct a new ProximalAdagrad optimizer.\n",
       "\n",
       "Args:\n",
       "  learning_rate: A `Tensor` or a floating point value.  The learning rate.\n",
       "  initial_accumulator_value: A floating point value.\n",
       "    Starting value for the accumulators, must be positive.\n",
       "  l1_regularization_strength: A float value, must be greater than or\n",
       "    equal to zero.\n",
       "  l2_regularization_strength: A float value, must be greater than or\n",
       "    equal to zero.\n",
       "  use_locking: If `True` use locks for update operations.\n",
       "  name: Optional name prefix for the operations created when applying\n",
       "    gradients.  Defaults to \"Adagrad\".\n",
       "\n",
       "Raises:\n",
       "  ValueError: If the `initial_accumulator_value` is invalid.\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda\\envs\\tsf36\\lib\\site-packages\\tensorflow\\python\\training\\proximal_adagrad.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tf.train.ProximalAdagradOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self, num_epoch=1000, batch_size=100, learning_rate=0.0005, momentum_rate=0.9, epochs_to_draw=10, reg=0.00015, num_train=1000, visualize=True):\n",
    "        self.num_epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum_rate = momentum_rate\n",
    "        self.epochs_to_draw = epochs_to_draw\n",
    "        self.reg = reg\n",
    "        self.num_train = num_train\n",
    "        self.visualize = visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, w_shape, activation, reg = 1e-5):\n",
    "        \n",
    "        mean = 0\n",
    "        std = 1\n",
    "        self.w = np.random.normal(0, np.sqrt(2./np.sum(w_shape)), w_shape)\n",
    "        self.activation = activation\n",
    "        self.reg = reg\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = np.dot(x, self.w)\n",
    "        \n",
    "        if self.activation == 'sigmoid':\n",
    "            result = sigmoid(result)\n",
    "        elif self.activation == 'relu':\n",
    "            result = reLU(result)\n",
    "        elif self.activation == 'tanh':\n",
    "            result = tanh(result)\n",
    "        elif self.activation == 'softmax':\n",
    "            result = softmax_minus_max(result)\n",
    "            \n",
    "    def backward(self, x, delta_prev):\n",
    "        if self.activation == 'sigmoid':\n",
    "            delta = delta_prev * sigmoid_grad(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tsf36]",
   "language": "python",
   "name": "conda-env-tsf36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
