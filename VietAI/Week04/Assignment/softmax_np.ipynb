{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04978707 0.13533528 0.36787944 1.        ]\n",
      " [0.04978707 0.01831564 0.36787944 1.        ]]\n",
      "[1.55300179 1.43598215]\n",
      "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.03467109 0.01275478 0.25618664 0.69638749]]\n"
     ]
    }
   ],
   "source": [
    "tmp = np.array([[1, 2, 3, 4], [5, 4, 7, 8]])\n",
    "tmp_derivative = tmp.copy()\n",
    "tmp_max = np.max(tmp, axis=1)\n",
    "for i in range(tmp.shape[1]):\n",
    "    tmp_derivative[:, i] = tmp[:, i] - tmp_max \n",
    "tmp_derivative = np.exp(tmp_derivative)\n",
    "print(tmp_derivative)\n",
    "tmp_s = np.sum(tmp_derivative, axis=1)\n",
    "print(tmp_s)\n",
    "\n",
    "tmp_hat = tmp_derivative.copy()\n",
    "for i in range(tmp.shape[0]):\n",
    "    tmp_hat[i, :] = tmp_hat[i, :] / tmp_s[i]\n",
    "print(tmp_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4,  9, 16],\n",
       "       [ 1,  4,  9, 16],\n",
       "       [ 1,  4,  9, 16],\n",
       "       [ 1,  4,  9, 16]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_a = np.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "tmp_b = np.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "tmp_a * tmp_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_a = np.array([[0 ,0, 0, 1, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0]])\n",
    "tmp_index = []\n",
    "for row in tmp_a:\n",
    "    tmp_index.extend([element_index for element_index in range(len(row)) if row[element_index] == 1])\n",
    "tmp_index = [element_index for element_index in range(len(tmp_index)) if tmp_index[element_index]==1]\n",
    "tmp_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tmp_a = np.array([[0.5 ,0.5, 0, 0.75, 0.3], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0]])\n",
    "\n",
    "for row in tmp_a:\n",
    "    print(np.argmax(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import get_mnist_data\n",
    "from logistic_np import add_one, LogisticClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading fashion MNIST data...\n",
      "Done reading\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(LogisticClassifier):\n",
    "    def __init__(self, w_shape):\n",
    "        super(SoftmaxClassifier, self).__init__(w_shape)\n",
    "    def feed_forward(self, x):\n",
    "        z = np.dot(x, self.w)\n",
    "        z_derivative = z.copy()\n",
    "        z_max = np.max(z, axis=1)\n",
    "        \n",
    "        for i in range(z.shape[1]):\n",
    "            z_derivative[:, i] = z[:, i] - z_max\n",
    "        z_derivative = np.exp(z_derivative)\n",
    "        \n",
    "        s = np.sum(z_derivative, axis=1)\n",
    "        \n",
    "        y_hat = z_derivative.copy()\n",
    "        for i in range(y_hat.shape[0]):\n",
    "            y_hat[i, :] = y_hat[i, :] / s[i]\n",
    "        return y_hat\n",
    "    \n",
    "    def compute_loss(self, y, y_hat):\n",
    "        return -np.sum(y * np.log(y_hat))/float(y.shape[0])\n",
    "    \n",
    "    def get_grad(self, x, y, y_hat):\n",
    "        return np.dot(x.T, (y_hat - y))/float(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train_x, test_x, val_x):\n",
    "    mean = np.sum(train_x)/float(train_x.shape[0] * train_x.shape[1])\n",
    "    std = np.sqrt(np.sum(np.square(train_x - mean))/float(train_x.shape[0] * train_x.shape[1]))\n",
    "\n",
    "    train_x = (train_x - mean)/float(std)\n",
    "    test_x = (test_x - mean)/float(std)\n",
    "    val_x = (val_x - mean)/float(std)\n",
    "    return train_x, val_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(labels, num_k=10):\n",
    "    one_hot_label = []\n",
    "    for label in labels:\n",
    "        tmp = [0] * num_k\n",
    "        tmp[label] = 1\n",
    "        one_hot_label.append(tmp)\n",
    "    \n",
    "    return np.array(one_hot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(y_hat, test_y):\n",
    "    confusion_mat_1 = np.zeros((10,10), dtype=np.float64)\n",
    "    \n",
    "    test_y_without_onehot = []\n",
    "    y_hat_without_onehot = []\n",
    "    \n",
    "    for row in test_y:\n",
    "        test_y_without_onehot.append(np.argmax(row))\n",
    "    \n",
    "    for row in y_hat:\n",
    "        y_hat_without_onehot.append(np.argmax(row))\n",
    "     \n",
    "    occur = np.bincount(test_y_without_onehot)\n",
    "    # print(occur)\n",
    "    # print(len(test_y_without_onehot))\n",
    "#     print(y_hat_without_onehot)\n",
    "    \n",
    "    confusion_mat = confusion_matrix(test_y_without_onehot, y_hat_without_onehot)\n",
    "    #print(confusion_mat)\n",
    "    #print(occur)\n",
    "    for row_index in range(confusion_mat.shape[0]):\n",
    "        confusion_mat_1[row_index, :] = confusion_mat[row_index, :]/float(occur[row_index])\n",
    "        \n",
    "    print(confusion_mat_1)\n",
    "#     for i in range(10):\n",
    "#         test_y_i = test_y_without_onehot[element_index for element_index in range(len(test_y_without_onehot)) \\\n",
    "#                                          if test_y_without_onehot[element_index]==i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = -10e-3\n",
    "abs(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 2.93782 || val loss: 3.07679\n",
      "Epoch 11: train loss: 1.51064 || val loss: 3.06391\n",
      "Epoch 21: train loss: 1.16940 || val loss: 3.42843\n",
      "Epoch 31: train loss: 1.01900 || val loss: 3.72563\n",
      "Epoch 41: train loss: 0.93101 || val loss: 3.95923\n",
      "Epoch 51: train loss: 0.87158 || val loss: 4.14950\n",
      "Epoch 61: train loss: 0.82790 || val loss: 4.30955\n",
      "Epoch 71: train loss: 0.79395 || val loss: 4.44755\n",
      "Epoch 81: train loss: 0.76648 || val loss: 4.56882\n",
      "Epoch 91: train loss: 0.74359 || val loss: 4.67698\n",
      "[[0.80152672 0.01526718 0.01526718 0.07633588 0.00763359 0.01908397\n",
      "  0.05725191 0.         0.00763359 0.        ]\n",
      " [0.0078125  0.91796875 0.00390625 0.046875   0.00390625 0.00390625\n",
      "  0.015625   0.         0.         0.        ]\n",
      " [0.00816327 0.00408163 0.64897959 0.         0.15510204 0.04897959\n",
      "  0.10612245 0.         0.02857143 0.        ]\n",
      " [0.05063291 0.05063291 0.00843882 0.82700422 0.01687764 0.00421941\n",
      "  0.03375527 0.         0.00843882 0.        ]\n",
      " [0.         0.02352941 0.13333333 0.05098039 0.69411765 0.01960784\n",
      "  0.07058824 0.         0.00784314 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.77108434\n",
      "  0.00803213 0.15261044 0.01606426 0.05220884]\n",
      " [0.2033195  0.00829876 0.17012448 0.04149378 0.18257261 0.03319502\n",
      "  0.34024896 0.         0.02074689 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.12295082\n",
      "  0.         0.79508197 0.         0.08196721]\n",
      " [0.00389105 0.0077821  0.0155642  0.02723735 0.0155642  0.05836576\n",
      "  0.0077821  0.0155642  0.84824903 0.        ]\n",
      " [0.         0.         0.         0.00393701 0.         0.02362205\n",
      "  0.         0.07086614 0.         0.9015748 ]]\n"
     ]
    }
   ],
   "source": [
    "train_y = create_one_hot(train_y)\n",
    "val_y = create_one_hot(val_y)\n",
    "test_y = create_one_hot(test_y)\n",
    "\n",
    "train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n",
    "\n",
    "num_feature = train_x.shape[1]\n",
    "dec_classifier = SoftmaxClassifier((num_feature, 10))\n",
    "momentum = np.zeros_like(dec_classifier.w)\n",
    "\n",
    "num_epoch = 100\n",
    "learning_rate = 0.01\n",
    "momentum_rate = 0.9\n",
    "epochs_to_draw = 10\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "for e in range(num_epoch):    \n",
    "        train_y_hat = dec_classifier.feed_forward(train_x)\n",
    "        val_y_hat = dec_classifier.feed_forward(val_x)\n",
    "\n",
    "        train_loss = dec_classifier.compute_loss(train_y, train_y_hat)\n",
    "        val_loss = dec_classifier.compute_loss(val_y, val_y_hat)\n",
    "\n",
    "        grad = dec_classifier.get_grad(train_x, train_y, train_y_hat)\n",
    "        \n",
    "        dec_classifier.update_weight(grad, learning_rate)\n",
    "        \n",
    "        all_train_loss.append(train_loss) \n",
    "        all_val_loss.append(val_loss)\n",
    "        \n",
    "        if e % 10 == 0:\n",
    "            print(\"Epoch %d: train loss: %.5f || val loss: %.5f\" % (e+1, train_loss, val_loss))\n",
    "\n",
    "test_y_hat = dec_classifier.feed_forward(train_x)\n",
    "test(test_y_hat, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from util import get_mnist_data\n",
    "from logistic_np import add_one\n",
    "from softmax_np import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading fashion MNIST data...\n",
      "Done reading\n",
      "Epoch 1: loss is 0.30646\n",
      "Epoch 101: loss is 0.14849\n",
      "Epoch 201: loss is 0.11653\n",
      "Epoch 301: loss is 0.10238\n",
      "Epoch 401: loss is 0.09403\n",
      "Epoch 501: loss is 0.08834\n",
      "Epoch 601: loss is 0.08411\n",
      "Epoch 701: loss is 0.08079\n",
      "Epoch 801: loss is 0.07808\n",
      "Epoch 901: loss is 0.07580\n",
      "Confusion matrix:\n",
      "[[0.85 0.04 0.02 0.04 0.02 0.   0.04 0.   0.   0.  ]\n",
      " [0.   0.93 0.   0.07 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.04 0.   0.43 0.04 0.26 0.   0.22 0.   0.   0.  ]\n",
      " [0.05 0.03 0.03 0.72 0.07 0.   0.1  0.   0.   0.  ]\n",
      " [0.   0.   0.12 0.09 0.65 0.02 0.09 0.   0.02 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.83 0.   0.11 0.02 0.04]\n",
      " [0.36 0.   0.09 0.02 0.3  0.02 0.19 0.   0.02 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.1  0.   0.78 0.   0.12]\n",
      " [0.04 0.   0.   0.02 0.   0.04 0.   0.04 0.87 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.08 0.   0.02 0.   0.9 ]]\n",
      "Diagonal values:\n",
      "[0.85 0.93 0.43 0.72 0.65 0.83 0.19 0.78 0.87 0.9 ]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = get_mnist_data()\n",
    "\n",
    "train_y = create_one_hot(train_y)\n",
    "val_y = create_one_hot(val_y)\n",
    "test_y = create_one_hot(test_y)\n",
    "\n",
    "train_x, val_x, test_x = normalize(train_x, val_x, test_x)\n",
    "    \n",
    "train_x = add_one(train_x) \n",
    "val_x = add_one(val_x)\n",
    "test_x = add_one(test_x)\n",
    "\n",
    "x = tf.placeholder(tf.float64, name='x')\n",
    "y = tf.placeholder(tf.float64, name='y')\n",
    "\n",
    "w_shape = (train_x.shape[1], 10)\n",
    "w = tf.Variable(np.random.normal(0, np.sqrt(2.0/np.sum(w_shape)), w_shape), name='weights')\n",
    "\n",
    "numerator = tf.exp(tf.matmul(x, w))\n",
    "denominator = tf.reduce_sum(numerator, 1, keepdims=True)\n",
    "pred = tf.divide(numerator, denominator)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(pred))\n",
    "\n",
    "num_epoch = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "epochs_to_draw = 1\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        sess.run(optimizer, feed_dict = {x: train_x, y: train_y})\n",
    "        # [TODO 1.16] Compute loss and update weights here\n",
    "        loss = sess.run(cost, feed_dict = {x: train_x, y: train_y})\n",
    "        # Update weights...\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "            print(\"Epoch %d: loss is %.5f\" % (e+1, loss))\n",
    "        \n",
    "    y_hat = sess.run(pred, feed_dict={x: test_x})\n",
    "    test(y_hat, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m Computes Python style division of `x` by `y`.\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\anaconda\\envs\\tsf36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tf.divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tsf36]",
   "language": "python",
   "name": "conda-env-tsf36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
